{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -r './requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "PINECONE_API_KEY = os.environ['PINECONE_API_KEY']\n",
    "PINECONE_API_ENV = os.environ['PINECONE_API_ENV']\n",
    "PINECONE_INDEX_NAME = os.environ['PINECONE_INDEX_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wells-fargo-demo'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PINECONE_INDEX_NAME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up OpenAI Embedding process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt, retry_if_not_exception_type\n",
    "from typing import List\n",
    "from uuid import uuid4\n",
    "import textwrap\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "EMBEDDING_MODEL = 'text-embedding-ada-002'\n",
    "EMBEDDING_CTX_LENGTH = 8191\n",
    "EMBEDDING_ENCODING = 'cl100k_base'\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6), retry=retry_if_not_exception_type(openai.InvalidRequestError))\n",
    "def get_embedding(text_or_tokens, model=EMBEDDING_MODEL):\n",
    "    return openai.Embedding.create(input=text_or_tokens, model=model)\n",
    "\n",
    "def chunk_text(text: str, max_chunk_size: int, overlap_size: int) -> List[str]:\n",
    "    \"\"\"Helper function to chunk a text into overlapping chunks of specified size.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + max_chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += max_chunk_size - overlap_size\n",
    "    return chunks\n",
    "\n",
    "def transform_record(record: dict) -> List[dict]:\n",
    "    \"\"\"Transform a single record as described in the prompt.\"\"\"\n",
    "    max_chunk_size = 500\n",
    "    overlap_size = 100\n",
    "    chunks = chunk_text(record, max_chunk_size, overlap_size)\n",
    "    transformed_records = []\n",
    "    recordId = str(uuid4())\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_id = f\"{recordId}-{i+1}\"\n",
    "        transformed_records.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'chunk_parent_id': recordId,\n",
    "            'chunk_text': chunk,\n",
    "            'vector' : get_embedding(chunk).get('data')[0]['embedding']\n",
    "            #'sparse_values': splade(chunk)\n",
    "        })\n",
    "    return transformed_records"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben.ogden/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pinecone\n",
    "\n",
    "index_name = PINECONE_INDEX_NAME\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_API_ENV  # may be different, check at app.pinecone.io\n",
    ")\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # if does not exist, create index\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine',\n",
    "        metadata_config={'indexed': ['unused']},\n",
    "        pod_type='p1.x1'\n",
    "    )\n",
    "# connect to index\n",
    "index = pinecone.Index(index_name)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/out.txt', 'r', encoding='ISO-8859-1') as f:\n",
    "    file = f.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate embeddings and Pickle the results to save money on OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunked_data = []\n",
    "chunk_array = transform_record(file)\n",
    "for chunk in chunk_array:\n",
    "    chunked_data.append(chunk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data and vectors offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Pickle the array\n",
    "with open('data/vector_data_500_100_sparse_no_splade.pickle', 'wb') as f:\n",
    "    pickle.dump(chunked_data, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from local to upsert to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/vector_data_500_100_sparse_no_splade.pickle', 'rb') as f:\n",
    "    vector_data = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format data to load to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_entries_for_pinecone(entries):\n",
    "    \"\"\"\n",
    "    Prepares an array of entries for upsert to Pinecone.\n",
    "    Each entry should have a 'vector' field containing a list of floats.\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for entry in entries:\n",
    "        vector = entry['vector']\n",
    "        id = entry.get('chunk_id', '')\n",
    "        metadata = entry.get('metadata', {'chunk_id': entry.get('chunk_id', ''),'parent_id': entry.get('chunk_parent_id', ''), 'chunk_text': entry.get('chunk_text', '')})\n",
    "        values = [v for v in vector]\n",
    "        # sparse_values = entry['sparse_values']\n",
    "        #vectors.append({'id': id, 'metadata': metadata, 'values': values, 'sparse_values': sparse_values})\n",
    "        vectors.append({'id': id, 'metadata': metadata, 'values': values})\n",
    "    return {'vectors': vectors, 'namespace': ''}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = prepare_entries_for_pinecone(vector_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsert vectors (sparse and dense) and metadata to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:23<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # this is our progress bar\n",
    "\n",
    "batch_size = 32  # process everything in batches of 32\n",
    "for i in tqdm(range(0, len(vectors['vectors']), batch_size)):\n",
    "    ids_batch = [id['id'] for id in vectors['vectors'][i:i+batch_size]]\n",
    "    embeds = [id['values'] for id in vectors['vectors'][i:i+batch_size]]\n",
    "    meta = [id['metadata'] for id in vectors['vectors'][i:i+batch_size]]\n",
    "    # sparse_values = [id['sparse_values'] for id in vectors['vectors'][i:i+batch_size]]\n",
    "    upserts = []\n",
    "    # loop through the data and create dictionaries for uploading documents to pinecone index\n",
    "    # for _id, sparse, dense, meta in zip(ids_batch, sparse_values, embeds, meta):\n",
    "    for _id,dense, meta in zip(ids_batch, embeds, meta):\n",
    "        upserts.append({\n",
    "            'id': _id,\n",
    "            # 'sparse_values': sparse,\n",
    "            'values': dense,\n",
    "            'metadata': meta\n",
    "        })\n",
    "    # upload the documents to the new hybrid index\n",
    "    index.upsert(upserts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Pinecone and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 8000\n",
    "\n",
    "def retrieve(query):\n",
    "    res = openai.Embedding.create(\n",
    "        input=[query],\n",
    "        engine=EMBEDDING_MODEL\n",
    "    )\n",
    "\n",
    "    # retrieve from Pinecone\n",
    "    xq = res['data'][0]['embedding']\n",
    "    #sq = splade(query)\n",
    "\n",
    "\n",
    "    # get relevant contexts\n",
    "    #res = index.query(xq, top_k=5, include_metadata=True, sparse_vector=sq)\n",
    "    res = index.query(xq, top_k=5, include_metadata=True)\n",
    "    contexts = [\n",
    "        x['metadata']['chunk_text'] for x in res['matches']\n",
    "    ]\n",
    "\n",
    "    # build our prompt with the retrieved contexts included\n",
    "    prompt_start = (\n",
    "        \"Answer the question based on the context below. If you cannot answer based on the context or general knowledge about Wells Fargo, truthfully answer that you don't know.\\n\\n\"+\n",
    "        \"Context:\\n\"\n",
    "    )\n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    # append contexts until hitting limit\n",
    "    for i in range(1, len(contexts)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(contexts)-1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
    "                prompt_end\n",
    "            )\n",
    "    return prompt\n",
    "\n",
    "def complete(prompt):\n",
    "    # query text-davinci-003\n",
    "    res = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=512,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    return res['choices'][0]['text'].strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain Memory for conversation chat style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "llm = OpenAI(\n",
    "\ttemperature=0,\n",
    "\topenai_api_key=OPENAI_API_KEY,\n",
    "\tmodel_name=\"text-davinci-003\"\n",
    ")\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm, \n",
    "    # We set a very low max_token_limit for the purposes of testing.\n",
    "    memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=650)\n",
    ")\n",
    "#conversation_with_summary.predict(input=\"Hi, what's up?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample query to Pinecone and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Wells Fargo & Company is a leading financial services company that\n",
      "has approximately $1.9 trillion in assets and provides banking,\n",
      "investment, mortgage, and consumer and commercial finance services to\n",
      "individuals, businesses, and institutions.\n"
     ]
    }
   ],
   "source": [
    "query =\"What is Wells Fargo?\"\n",
    "# first we retrieve relevant items from Pinecone\n",
    "query_with_contexts = retrieve(query)\n",
    "print(textwrap.fill(str(conversation_with_summary.predict(input=query_with_contexts))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear conversation memory if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_with_summary.memory.clear()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop to ask multiple questions and get answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed input:  WHO IS WELLS FARGO AND CAN YOU SUMMARIZE THEIR 2022 Q1 VS 2023 Q1 EARNINGS?\n",
      " Wells Fargo is a financial services company based in the United\n",
      "States. In the first quarter of 2022, Wells Fargo reported net income\n",
      "of $3.8 billion and diluted earnings per share of $0.91. In the first\n",
      "quarter of 2023, Wells Fargo reported net income of $5.0 billion and\n",
      "diluted earnings per share of $1.23, representing an increase of 31%\n",
      "in net income and 34% in diluted earnings per share compared to the\n",
      "same period a year ago.\n",
      "\n",
      "Processed input:  WHAT WERE THE TOTAL RISK WEIGHTED ASSETS IN 2023? HOW DID THEY COMPARE TO 2022?\n",
      " In 2023, Wells Fargo's total Risk Weighted Assets were $1.24\n",
      "trillion, representing a decrease of 1.3% compared to the total Risk\n",
      "Weighted Assets of $1.26 trillion in 2022.\n",
      "\n",
      "Processed input:  WHAT IS THE COMMON EQUITY TIER 1 (CET1) RATIO BUFFER FOR 2023?\n",
      " The Common Equity Tier 1 (CET1) ratio buffer for 2023 is 3.20%.\n",
      "\n",
      "Processed input:  CAN YOU TELL ME HOW Q2 IS LOOKING?\n",
      " Based on the context provided, Q2 of 2023 is forecasted to have a\n",
      "U.S. unemployment rate of 4.7%, a U.S. real GDP of -0.3%, a home price\n",
      "index of -5.5%, and a commercial real estate asset prices of -5.2%.\n",
      "\n",
      "Processed input:  HOW ARE THINGS LOOKING FOR 2024 AND BEYOND?\n",
      " Based on the context provided, Q4 of 2024 is forecasted to have a\n",
      "U.S. unemployment rate of 6.2%, a U.S. real GDP of 1.1%, a home price\n",
      "index of -6.2%, and a commercial real estate asset prices of -5.8%.\n",
      "Beyond 2024, I do not have enough information to provide an accurate\n",
      "answer.\n",
      "\n",
      "Processed input:  HOW IS 2025 LOOKING?\n",
      " I do not have enough information to provide an accurate answer for\n",
      "2025.\n",
      "\n",
      "Exiting program...\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Prompt user for input\n",
    "    user_input = input(\"Enter your input (type 'quit' to exit): \")\n",
    "\n",
    "    # Check if user wants to quit\n",
    "    if user_input.lower() == \"quit\":\n",
    "        print(\"Exiting program...\")\n",
    "        break\n",
    "\n",
    "    # Process user input\n",
    "    processed_input = user_input.upper()  # Convert to all uppercase letters\n",
    "    print(\"Processed input: \", processed_input)\n",
    "\n",
    "    query = user_input\n",
    "\n",
    "    # first we retrieve relevant items from Pinecone\n",
    "    query_with_contexts = retrieve(query)\n",
    "\n",
    "    # then we send the context and the query to OpenAI\n",
    "    print(textwrap.fill(str(conversation_with_summary.predict(input=query_with_contexts))) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
