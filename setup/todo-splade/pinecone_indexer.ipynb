{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -r './requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data file and process using pdf_reader.py:\n",
    "# https://s26.q4cdn.com/463892824/files/doc_financials/2023/q4/0d4be772-30ac-4017-a78b-a5eaa081840e.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('.env')\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "PINECONE_API_KEY = os.environ['PINECONE_API_KEY']\n",
    "PINECONE_API_ENV = os.environ['PINECONE_API_ENV']\n",
    "PINECONE_INDEX_NAME = os.environ['PINECONE_INDEX_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_INDEX_NAME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Embeddings with 500 tokens and 100 overlap (Sparse and Dense + SPLADE)\n",
    "#### SPLADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "class SPLADE:\n",
    "    def __init__(self, model):\n",
    "        # check device\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model)\n",
    "        # move to gpu if available\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "\n",
    "        inter = torch.log1p(torch.relu(logits[0]))\n",
    "        token_max = torch.max(inter, dim=0)  # sum over input tokens\n",
    "        nz_tokens = torch.where(token_max.values > 0)[0]\n",
    "        nz_weights = token_max.values[nz_tokens]\n",
    "\n",
    "        order = torch.sort(nz_weights, descending=True)\n",
    "        nz_weights = nz_weights[order[1]]\n",
    "        nz_tokens = nz_tokens[order[1]]\n",
    "        return {\n",
    "            'indices': nz_tokens.cpu().numpy().tolist(),\n",
    "            'values': nz_weights.cpu().numpy().tolist()\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate splade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splade = SPLADE(\"naver/splade-cocondenser-ensembledistil\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splade (sparse vector example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"what is the capital of france?\"\n",
    "sparse_vector = splade(doc)\n",
    "print(sparse_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up OpenAI Embedding process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt, retry_if_not_exception_type\n",
    "from typing import List\n",
    "from uuid import uuid4\n",
    "import textwrap\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "EMBEDDING_MODEL = 'text-embedding-ada-002'\n",
    "EMBEDDING_CTX_LENGTH = 8191\n",
    "EMBEDDING_ENCODING = 'cl100k_base'\n",
    "\n",
    "# let's make sure to not retry on an invalid request, because that is what we want to demonstrate\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6), retry=retry_if_not_exception_type(openai.InvalidRequestError))\n",
    "def get_embedding(text_or_tokens, model=EMBEDDING_MODEL):\n",
    "    return openai.Embedding.create(input=text_or_tokens, model=model)\n",
    "\n",
    "def chunk_text(text: str, max_chunk_size: int, overlap_size: int) -> List[str]:\n",
    "    \"\"\"Helper function to chunk a text into overlapping chunks of specified size.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + max_chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += max_chunk_size - overlap_size\n",
    "    return chunks\n",
    "\n",
    "def transform_record(record: dict) -> List[dict]:\n",
    "    \"\"\"Transform a single record as described in the prompt.\"\"\"\n",
    "    max_chunk_size = 500\n",
    "    overlap_size = 50\n",
    "    chunks = chunk_text(record, max_chunk_size, overlap_size)\n",
    "    transformed_records = []\n",
    "    recordId = str(uuid4())\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_id = f\"{recordId}-{i+1}\"\n",
    "        transformed_records.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'chunk_parent_id': recordId,\n",
    "            'chunk_text': chunk,\n",
    "            'vector' : get_embedding(chunk).get('data')[0]['embedding'],\n",
    "            'sparse_values': splade(chunk)\n",
    "        })\n",
    "    return transformed_records"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "index_name = PINECONE_INDEX_NAME\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_API_ENV  # may be different, check at app.pinecone.io\n",
    ")\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # if does not exist, create index\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,\n",
    "        metric='dotproduct',\n",
    "        metadata_config={'indexed': ['unused']},\n",
    "        pod_type='p1.x1'\n",
    "    )\n",
    "# connect to index\n",
    "index = pinecone.Index(index_name)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and load data (LOTR Fellowship of the Ring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/snowflake_10k_report.txt', 'r', encoding='ISO-8859-1') as f:\n",
    "    file = f.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate embeddings and Pickle the results to save money on OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunked_data = []\n",
    "chunk_array = transform_record(file)\n",
    "for chunk in chunk_array:\n",
    "    chunked_data.append(chunk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data and vectors offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Pickle the array\n",
    "with open('./data/Snowflake_vector_data.pickle_500_100_sparse_dense', 'wb') as f:\n",
    "    pickle.dump(chunked_data, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from local to upsert to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/Snowflake_vector_data.pickle_500_100_sparse_dense', 'rb') as f:\n",
    "    vector_data = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format data to load to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_entries_for_pinecone(entries):\n",
    "    \"\"\"\n",
    "    Prepares an array of entries for upsert to Pinecone.\n",
    "    Each entry should have a 'vector' field containing a list of floats.\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for entry in entries:\n",
    "        vector = entry['vector']\n",
    "        id = entry.get('chunk_id', '')\n",
    "        metadata = entry.get('metadata', {'chunk_id': entry.get('chunk_id', ''),'parent_id': entry.get('chunk_parent_id', ''), 'chunk_text': entry.get('chunk_text', '')})\n",
    "        values = [v for v in vector]\n",
    "        sparse_values = entry['sparse_values']\n",
    "        vectors.append({'id': id, 'metadata': metadata, 'values': values, 'sparse_values': sparse_values})\n",
    "    return {'vectors': vectors, 'namespace': ''}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = prepare_entries_for_pinecone(vector_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsert vectors (sparse and dense) and metadata to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # this is our progress bar\n",
    "\n",
    "batch_size = 32  # process everything in batches of 32\n",
    "for i in tqdm(range(0, len(vectors['vectors']), batch_size)):\n",
    "    ids_batch = [id['id'] for id in vectors['vectors'][i:i+batch_size]]\n",
    "    embeds = [id['values'] for id in vectors['vectors'][i:i+batch_size]]\n",
    "    meta = [id['metadata'] for id in vectors['vectors'][i:i+batch_size]]\n",
    "    sparse_values = [id['sparse_values'] for id in vectors['vectors'][i:i+batch_size]]\n",
    "    upserts = []\n",
    "    # loop through the data and create dictionaries for uploading documents to pinecone index\n",
    "    for _id, sparse, dense, meta in zip(ids_batch, sparse_values, embeds, meta):\n",
    "        upserts.append({\n",
    "            'id': _id,\n",
    "            'sparse_values': sparse,\n",
    "            'values': dense,\n",
    "            'metadata': meta\n",
    "        })\n",
    "    # upload the documents to the new hybrid index\n",
    "    index.upsert(upserts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Pinecone and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 8000\n",
    "\n",
    "def retrieve(query):\n",
    "    res = openai.Embedding.create(\n",
    "        input=[query],\n",
    "        engine=EMBEDDING_MODEL\n",
    "    )\n",
    "\n",
    "    # retrieve from Pinecone\n",
    "    xq = res['data'][0]['embedding']\n",
    "    sq = splade(query)\n",
    "\n",
    "\n",
    "    # get relevant contexts\n",
    "    res = index.query(xq, top_k=5, include_metadata=True, sparse_vector=sq)\n",
    "    contexts = [\n",
    "        x['metadata']['chunk_text'] for x in res['matches']\n",
    "    ]\n",
    "\n",
    "    # build our prompt with the retrieved contexts included\n",
    "    prompt_start = (\n",
    "        \"Answer the question based on the context below. If you cannot answer based on the context or general knowledge about the company Snowflake, truthfully answer that you don't know.\\n\\n\"+\n",
    "        \"Context:\\n\"\n",
    "    )\n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    # append contexts until hitting limit\n",
    "    for i in range(1, len(contexts)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(contexts)-1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
    "                prompt_end\n",
    "            )\n",
    "    return prompt\n",
    "\n",
    "def complete(prompt):\n",
    "    # query text-davinci-003\n",
    "    res = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=512,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    return res['choices'][0]['text'].strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain Memory for conversation chat style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "llm = OpenAI(\n",
    "\ttemperature=0,\n",
    "\topenai_api_key=OPENAI_API_KEY,\n",
    "\tmodel_name=\"text-davinci-003\"\n",
    ")\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm, \n",
    "    # We set a very low max_token_limit for the purposes of testing.\n",
    "    memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=650)\n",
    ")\n",
    "#conversation_with_summary.predict(input=\"Hi, what's up?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample query to Pinecone and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"Who are Snowflake's competitors?\"\n",
    "# first we retrieve relevant items from Pinecone\n",
    "query_with_contexts = retrieve(query)\n",
    "print(textwrap.fill(str(conversation_with_summary.predict(input=query_with_contexts))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear conversation memory if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversation_with_summary.memory.clear()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop to ask multiple questions and get answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Prompt user for input\n",
    "    user_input = input(\"Enter your input (type 'quit' to exit): \")\n",
    "\n",
    "    # Check if user wants to quit\n",
    "    if user_input.lower() == \"quit\":\n",
    "        print(\"Exiting program...\")\n",
    "        break\n",
    "\n",
    "    # Process user input\n",
    "    processed_input = user_input.upper()  # Convert to all uppercase letters\n",
    "    print(\"Processed input: \", processed_input)\n",
    "\n",
    "    query = user_input\n",
    "\n",
    "    # first we retrieve relevant items from Pinecone\n",
    "    query_with_contexts = retrieve(query)\n",
    "\n",
    "    # then we send the context and the query to OpenAI\n",
    "    print(textwrap.fill(str(conversation_with_summary.predict(input=query_with_contexts))) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinecone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
